---
title: Y-Trainer 介绍
description: Y-Trainer - 智能大模型训练工具
---

## Y-Trainer 介绍

Y-Trainer 是一个能够有效避免模型灾难性遗忘和过拟合的训练框架。

> 首创的NLIRG算法（以下简称Y-Trainer算法）可以实现Token级别动态计算，调整训练梯度，帮助模型再困难样本和简单样本之间，平滑的调整学习强度。

集成了继续预训练(CPT)、指令微调(SFT)和强化学习(RL)三大核心功能。

## 📊 优势简介

- 📉 精准对抗过拟合： 专门优化，有效解决SFT中的过拟合难题。

- 🧩 突破遗忘瓶颈： 无需依赖通用语料，即可卓越地保留模型的泛化能力，守住核心能力的同时实现专项提升！

- 🏆 单卡强化学习：无需依赖参考模型、教师模型，仅需基础模型+奖励函数，即可稳定的进行强化学习训练。

### 🚀 Y-Trainer算法特点（NLIRG）

- **避免灾难遗忘** ： 灾难性遗忘通常是由过难语料导致，通过识别这些token，进行动态调整，可有效避免 。
- **防止过拟合** ： 过拟合是由相似语料或者模型已经掌握的知识导致，通过识别这些token，进行动态调整，可有效避免 过拟合问题。
- **识别问题语料** ： Y-Trainer算法通过模型内部信号，可以对语料进行质量评分，提早排查错误。
- **无需通用语料** ： 传统的SFT通常需要混合一定比例通用语料，防止模型能力退化，Y-Trainer算法可在只使用垂直领域语料的情况下训练，并取得更好的效果。
- **无需语料平衡** ： 传统的SFT通常需要平和不同语料的分布，我们的Y-Trainer算法，即使在语料分布很不均匀的情况下，依然能够稳定训练。

### 框架优势

| 特性 | 描述 | 价值 |
|------|------|------|
| **智能训练优化** | 内置Y-Trainer算法，根据样本难度动态调整训练强度 | 提升训练效率，防止过拟合 |
| **资源友好** | 支持LoRA训练和多卡并行，显著降低硬件要求 | 让更多用户能够训练大模型 |
| **易用性强** | 完整的配置管理和TensorBoard可视化支持 | 简化训练流程，便于监控 |
| **模型兼容性好** | 支持Qwen等主流大语言模型 | 灵活适配不同场景需求 |


### 💬 SFT：指令微调

**功能概述**：让模型学会遵循特定指令格式，完成结构化任务，而无需通用语料。

**核心优势**：

| 优势 | 传统SFT | Y-Trainer SFT |
|------|---------|---------------|
| **能力保持** | 容易遗忘原有能力 | ✅ 智能保护基座模型能力 |
| **训练效率** | 需要复杂的数据平衡 | ✅ 无需数据平衡，快速收敛 |
| **资源需求** | 显存要求高 | ✅ 支持LoRA，显著降低需求 |
| **训练优化** | 固定训练强度 | ✅ 动态调整，智能优化 |

**适用任务**：问答、摘要、翻译、代码生成、角色扮演等


### 📚 CPT：继续预训练

**功能概述**：通过领域知识增强，让模型在保持原有能力的同时学习新知识。

**适用场景**：
- ✅ 领域迁移：让模型适应新的专业领域
- ✅ 知识补充：为模型添加特定领域的专业知识
- ✅ 能力扩展：提升模型在特定任务上的表现

**技术特点**：
- 支持大规模文本数据的预训练
- 智能优化训练过程，避免知识遗忘
- 可针对不同领域进行定制化训练

### 🎯 RL：强化学习(开发中，需要配合Y-Agent框架)

**功能概述**：基于SFT构建的轻量级强化学习框架，让模型通过交互学习最优策略。

**技术突破**：

```
传统RL框架：
模型 → 参考模型 → 奖励模型 → 价值网络 → 训练

Y-Trainer RL框架：
模型 → 智能探索 → 自适应训练 → 优化策略
```

**核心优势**：
- 🚀 **资源极简**：无需复杂组件，仅需设计奖励函数
- 🛡️ **训练稳定**：智能探索策略，避免训练崩溃
- 📈 **效率提升**：高熵token引导，确保有效学习


> 计划：很快开源

### 🔧 实用工具

**指令微调语料训练顺序调整工具**

**解决的问题**：传统训练中语料顺序随机，影响学习效率

**解决方案**：
- 📊 **智能评估**：分析模型对语料的响应模式
- 🔄 **自动排序**：按照从易到难的顺序排列语料
- 🎯 **渐进学习**：让模型逐步掌握复杂任务

**效果**：训练效率提升30%+，模型性能更稳定

## 🚀 快速开始指南

### 选择训练类型

| 训练类型 | 适用场景 | 建议轮次 | 关键参数 |
|----------|----------|----------|----------|
| **CPT** | 领域迁移、知识补充 | 3-5轮 | `--training_type cpt` |
| **SFT** | 指令遵循、任务完成 | 2-4轮 | `--training_type sft` |
| **RL** | 策略优化、交互学习 | 根据任务复杂度 | `--training_type rl` |

### 资源优化建议

**显存不足时**：
- 启用LoRA训练：`--use_lora`
- 使用梯度检查点：`--enable_gradit_checkpoing`
- 减小批次大小：`--batch_size 1`

**追求最佳效果**：
- 启用NLIRG算法：`--use_NLIRG`
- 使用多卡训练：`--use_deepspeed`
- 启用可视化：`--use_tensorboard`

> 💡 **提示**：详细配置说明请参考[配置文档](./config)